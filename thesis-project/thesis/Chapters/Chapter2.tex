\chapter{Background and Related Work}
\label{Chapter2}


\section{Code readability}

Code readability stands as a pivotal element in software maintenance, a fact supported by ample evidence in the literature. Tufano et al. \cite{Tufano2015} delineate various factors indicating when code quality deteriorates, commonly referred to as "code smells." The study shows that code smells are often introduced upon file creation or repeated modification of the same file. It is demonstrated that the file creator often introduces them, and this typically occurs months before a deadline or when the developer is under heavy workload. Sedano et al. \cite{Sedano2016} illustrate that developers are often unaware of the difficulty another developer faces in reading the code they produce, and how this awareness eventually drives developers to write more readable code, whether the initial code was readable or not. Scalabrino et al. \cite{Scalabrino2018} propose a model aimed at estimating code readability by improving upon previous state-of-the-art models. These improvements are based on previously unexplored textual features such as Comments and Identifiers consistency, which measures the overlap of terms used in comments with those used in the code body, and Identifier terms in dictionary, a metric aiming to identify how many of the words used belong to the English dictionary, as it has been shown that abbreviated naming is often correlated with lower readability.\newline
Understanding these indicators is crucial, although their effectiveness is questioned by the study conducted by Fakhoury et al. \cite{Fakhoury2019}, demonstrating that these metrics may be partially ineffective in capturing what a developer would define as an improvement in code readability. The study shows that the existing models are unable to capture improvements in readability during software maintenance and proposes additional metrics to enhance their effectiveness. Additionally, Travares et al. \cite{Travares2020} shed light on the intricacy of this issue, cautioning that attempts to enhance code readability through automated refactoring can inadvertently worsen the situation. Indeed, it has been shown that automated refactoring practices may introduce code smells. Another pivotal study conducted by Oliveira et al. \cite{Oliviera2020} exposes some shortcomings in evaluation studies on readability conducted using human subjects, revealing that different approaches require different skills and that only some of these skills are utilized in these evaluations.

\section{Machine learning to improve code readability}

Enhancing code readability using machine learning models represents a dynamic area of research, with several studies showcasing their efficacy in automating refactoring tasks. For instance, Piantadosi \cite{Piantadosi2020} demonstrated how software tends to evolve and lose readability to a relevant extent as features are added and changes are made to it, and outlined guidelines such as small incremental commits and frequent refactoring practices to mitigate this issue. This observation underscores the importance of the study mentioned in the preceding paragraph, which investigates the efficacy of deep learning techniques in enhancing code readability. By addressing this issue, the study contributes to mitigating the potential loss of readability in evolving software systems. \newline
Regarding the automation of refactoring activities, some studies have focused on automatically improving certain characteristics that describe more readable code. The study conducted by Mastropaolo et al. \cite{Mastropaolo2023} demonstrates how deep learning models can be applied to automate variable renaming as a technique to enhance readability. Finally, Vitale et al. \cite{Vitale2023}, the study that profoundly influenced our research direction, significantly advanced the field of automated refactoring. This study introduced a deep learning model capable of suggesting optimal actions (such as renaming, code extraction, styling operations, etc.) to enhance code readability. By training a neural network on a dataset comprising 122k commits, this approach effectively modified code without altering its behavior in 69\% of cases and improved readability in Java code snippets by 79.4\%. These findings served as the foundation for our research, which aimed to apply this model in real-world application contexts.




